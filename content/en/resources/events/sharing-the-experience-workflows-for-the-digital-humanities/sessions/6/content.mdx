## Abstract

We present our method and interim results of the "Mining Goodreads" project, aimed at developing a computational approach to measure reading absorption in user-generated book reviews in English. Annotation of 600 texts showed the difficulties in finding an agreement in the tagging of sentences. However, the parallel work of five annotators offered the opportunity to distant read the language used by reviewers when talking about absorption. Machine learning approaches were applied on the annotated corpus, producing promising results.
