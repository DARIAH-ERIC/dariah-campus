---
title: A Practical Introduction to Knowledge Graphs
locale: en
publication-date: 2025-07-08
version: 1.0.0
authors:
  - pertsas-vayianos
editors: []
contributors: []
tags:
  - data-modeling
  - machine-learning
  - semantic-web
  - data-management
  - artificial-intelligence
sources:
  - dariah
license: cc-by-4.0
table-of-contents: true
summary:
  title: A Practical Introduction to Knowledge Graphs
  content: >-
    The course offers an extensive introduction to the topic of Knowledge
    Graphs. We explore their basic principles, demonstrate the benefits of their
    use and examine various digital methods for their design and creation from
    text. Specifically, following a real use-case scenario -that of modeling and
    transforming Humanities research articles into a scholarly knowledge graph-
    we examine the application of various digital methods involved in the entire
    workflow concerning the design, creation and use of Knowledge Graphs: from
    organizing and representing domain knowledge, to textual data processing,
    information extraction, entity disambiguation and linking as well as linked
    data generation and query formulation.
content-type: training-module
translations: []
dariah-national-consortia: []
---
## Description

The course offers an extensive introduction to the topic of Knowledge Graphs. We explore their basic principles, demonstrate the benefits of their use and examine various digital methods for their design and creation from text. Specifically, following a real use-case scenario -that of modeling and transforming Humanities research articles into a scholarly knowledge graph- we examine the application of various digital methods involved in the entire workflow concerning the design, creation and use of Knowledge Graphs: from organizing and representing domain knowledge, to textual data processing, information extraction, entity disambiguation and linking as well as linked data generation and query formulation.

## Objectives

Develop a practical understanding of how diverse methods and techniques converge in the construction of Knowledge Graphs. This includes grasping the core principles, functions, and value of Knowledge Graphs, as well as engaging with the broader ecosystem of digital methods that support their creation. Learners will examine how knowledge representation within the Semantic Web provides a conceptual and technical foundation; how text processing and manipulation enable the transformation of unstructured content; how entity extraction, linking, and disambiguation contribute to building coherent semantic structures; and how linked data generation and SPARQL querying facilitate the production, interconnection, and retrieval of machine-readable knowledge. Along this line, the central objective is to understand and apply the full synthetic process through which these components interact to form robust, meaningful, and interoperable Knowledge Graphs.

## Learning Outcomes

Upon completion of the course, participants are expected to be able to:
•	Understand the main idea and benefits of using Knowledge Graphs.
•	Assess the basic concepts behind the Semantic Web and the benefits of representing knowledge as linked data.
•	Assess the core ideas behind RDF and RDFS data models in order to work with knowledge representation tasks in the Semantic Web.
•	Design Knowledge Graph schemas in Python using the RDFS data model.
•	Employ Machine Learning models for Entity Extraction using SpaCy NLP framework.
•	Disambiguate extracted entities using Public Knowledge Bases such as Wikipedia.
•	Create semantic relationships among the extracted entities using inference rules.
•	Combine information from different sources (metadata, text) in order to build a Knowledge Graph.
•	Publish Knowledge Graphs as linked data for the Semantic Web.
•	Perform basic SPARQL queries in order to retrieve information from an RDF Knowledge Graph.

## Prerequisites

Students must be familiarized with Python Programming Language.
In addition, there is extended use of the Python libraries: [RDFLib](https://rdflib.readthedocs.io/en/stable/), as well as [SpaCy](https://spacy.io/) NLP framework.
This course utilizes digital methods and tools from various computer science topics such as conceptual modeling, RDFS, Machine Learning, entity extraction, entity disambiguation, SPARQL. Each mentioned topic is briefly reviewed and explained to make the course comprehensive and self-sufficient. Nevertheless, for more in-depth understanding, additional bibliography and resources will be provided on each occasion.

---

## Designing Knowledge Graphs for the Semantic Web

### Introduction

In this section we will delve into the world of Knowledge Graphs and the representation of knowledge in a structured -machine comprehendible- way. Specifically, we will focus on designing Knowledge Graphs for the Semantic Web. We will explore their role and usage in knowledge representation, we will explain their relationship with conceptual models like ontologies and familiarize ourselves with the RDF / RDFS languages for designing our models and the SPARQL language for querying our knowledge graphs. Finally, we will go hands on with building a schema for a knowledge graph using the RDFLib Python Library. So, let’s start from the beginning by answering the most important question:

### What is a Knowledge Graph?

A Knowledge Graph (KG) is an organized representation of data in the form of entities and their relationships. Entities can represent various concepts (such as real objects, events, persons, or types of entities), while the relationships among them capture their semantic context, meaning they describe how they are connected. Typically, KGs are implemented in a form of Directed Labeled Graph, meaning a structure of interconnected i) nodes and ii) links among them. Both nodes and links bear a label that is used for describing their semantic meaning in natural language. In addition, the links can bear a direction, that describes how their semantic meaning is applied (i.e. from which node it starts and to which node it ends).&#x20;

As an example, consider the phrase: “Parthenon is located in Athens”. Using the above principles, we could represent the knowledge encoded in the above phrase as a directed labeled graph, where two nodes represent the Parthenon and Athens and a relationship *isLocatedIn(Parthenon,Athens)* links Parthenon to Athens based on the semantics declared by its label. This transformation is shown in the following figure.

<Figure src="/assets/content/assets/en/resources/hosted/a-practical-introduction-to-knowledge-graphs/sample-triple.png" alt="sample triple transformation" alignment="stretch">

</Figure>

### Why Knowledge Graphs?

The usefulness of Knowledge Graphs (KGs) lies in the way they organize data in the form of a network of interconnected concepts. This inherent structure allows for easy exploration and retrieval of information (by traversing the graph) as well as the inference of new knowledge through the application of inference rules, network analysis algorithms, graph embeddings etc. In addition, when combined with Large Language Models such as Chat-GPT, KGs can help increase response accuracy and improve explainability with the context provided by data relationships. This is achieved, using Graph-based Retrieval Augmented Generation (Graph-RAG), a technique that grounds LLMs with domain knowledge. For the above reasons, KGs can be very useful in many use cases of real-world applications such as Fraud Detection and Analytics in Financial Services, Banking and Insurance, investigative journalism, drug discovery in healthcare research, recommender systems, etc. In fact, the entire web can be considered as a huge knowledge graph with websites and digital resources linked to each other. Access to that knowledge graph is witnessed every time we use our familiar search engines like google search:

<Figure src="/assets/content/assets/en/resources/hosted/a-practical-introduction-to-knowledge-graphs/google-search.png" alt="google sample search" alignment="stretch">

</Figure>

The <Link link={{"discriminant":"external"}}>[Google Knowledge Graph](https://blog.google/products/search/introducing-knowledge-graph-things-not/)</Link> compiles facts about people, places, and things into an interconnected web of entities. During a Google search, it leverages these connections between entities to provide relevant results within context, displayed as the "[knowledge panel](https://support.google.com/knowledgepanel/answer/9163198?hl=en)". The entities in the knowledge graph reflect our understanding of the world, representing a shift from viewing data as mere text ("strings") to organized entities ("things"). This transformation allows Google to utilize the collective intelligence embedded in this network to deliver search results that align with the intent behind your query rather than just matching keywords.

### Ontologies as Blueprints for Knowledge Graphs

As mentioned above, Knowledge Graphs constitute organized representations of data in the form of entities and their relationships. This “organization” that provides the internal structure among the KG derives from a set of “organizing principles”, that serve as a framework or schema that arranges entities and their connections based on core concepts, crucial for specific use cases. Unlike many other data structures, knowledge graphs can accommodate multiple organizing principles. Consider an organizing principle as a conceptual map or *metadata layer* that provides the schema of the structure of the actual data of the KG and is superimposed on its instances which -along with their connections- constitute the *instance layer*.

Along the same line, an ontology in computer science, is defined as a formal specification of concepts and their interconnections within a specific domain of knowledge; this definition makes ontologies an ideal candidate to act as the blueprint that provides the organizing principles for knowledge graphs. The schema of the KG follows the same entity-relationship structure found throughout the knowledge graph instances, allowing queries to draw information from both instance and schema layer.

### The Semantic Web

The most prominent endeavor to apply the above concepts for representing knowledge at a global scale is the Semantic Web. The main idea behind it is to make the web more accessible to computers. Indeed, the internet can be considered as a web of interconnected resources such as text, images, sounds, videos, etc. However, without actual representation of the content that is exchanged, all these resources become black boxes for computers that, apart from indexing and transferring information from servers to clients, have very limited use in assisting humans do the actual hard work, that of aggregate, combine, select, and comprehend content. In order to solve this issue, the philosophy of Semantic Web was built upon three basic ideas:

* Make structured and semi-structured data available in standardized formats on the web.&#x20;
* Make not just the datasets but their individual data elements and their relations accessible on the web.&#x20;
* Describe the intended semantics of such data in a formalism, so that it can be processed by machines.

In order to apply these ideas, the vision of Semantic Web provided the following technology guidelines:

* Use labeled Graphs, <Footnote>a labeled graph can be considered a “web” of nodes that are interconnected through various relationships, where each node or relationship can be assigned a specific “name” via a label in order to describe its function (semantics)</Footnote> to model objects and their relations.&#x20;
* Use web identifiers <Footnote>A web identifier is a unique sequence of characters that is used to identify an object in a domain. A special case of URI is the Uniform Resource Locator (URL) that is used to uniquely identify the address of a web page over the internet. The difference with a URI is that the naming in a URI is more general and can contain components like a scheme, authority, path, and query. URL has similar components to a URI, but its authority consists of a domain name and port.</Footnote> (Uniform Resource Identifiers -URIs) to identify the individual data-items and their relations.&#x20;
* Use ontologies as the data model to formally represent the intended semantics of the data.

Simply put, Knowledge Graphs can be utilized in order to represent every concept that we need to talk about on the internet. Digital objects (text, video, image, sound, etc.) or parts thereof can be assigned a unique identifier that acts as their ID and be connected to each other through appropriate relationships that describe semantic context. This representation of entities and relationships is organized by schemas (i.e. ontologies) that provide the conceptual map -the blueprints- of those interconnections.

### Modeling Languages for the Semantic Web: RDF and RDFS

Adopting the above guidelines allows for transformation of the way information is shared and exchanged on the web. In order to do so, however, we need a specific language that makes this representation machine comprehendible and a syntax so that this representation is serializable and can be stored as a file. The language that was developed for representing KGs in the Semantic Web is called [Resource Description Framework (RDF)](https://www.w3.org/RDF/) and is based on the following principles:

* A Resource is every “object” or “thing” that we want to talk about.&#x20;
* Every resource has a URI. A URI can be a URL for the case of web pages or any other kind of unique identifier.&#x20;
* Properties are a special kind of resource: they describe relations between other resources ●	Like all resources, properties are also identified by URIs.&#x20;
* Statements are assertions regarding the properties of resources.&#x20;
* A statement is an entity-attribute-value triple, consisting of a resource, a property and a value.
* The value can be either a resource or a literal.&#x20;
* Literals are atomic values (i.e. numbers, strings, dates, currencies, etc.) and they are not assigned URIs.

Using the following rules, knowledge can be represented in the form of triples (subject, predicate, value) in RDF. The following figure shows how the previous statement in the form of a labeled graph can be transformed into an RDF triple:

<Figure src="/assets/content/assets/en/resources/hosted/a-practical-introduction-to-knowledge-graphs/sample-rdf-triple.png" alt="sample RDF triple" alignment="stretch">

</Figure>

This statement is in the form of resource-property-resource and consists of three URIs: two nodes and one property. So, a computer, parsing the above triple could understand that it is talking about two resources (the URLs of Parthenon and Athens from the Wikipedia and DBpedia websites) that are interrelated through the property of location as defined in its DBpedia URL respectively.

<Callout kind="tip">
  In this example, each URI is actually a URL i.e. a unique identifier denoting the location of the resource on the web (its web address). The first part of each URI (i.e. the part of the address until the name of the resource) represents the namespace, the space where all the resources are getting their names. In other words, the namespace Wikipedia ([https://en.wikipedia.org](https://en.wikipedia.org)) is where the name Parthenon is defined, while the namespace DBpedia ([http://dbpedia.org](http://dbpedia.org)) is where the resources location and Athens are defined. Through RDF any individual namespace can be interconnected based on this simple resource-property-value triple form.
</Callout>

Up until now we have managed to represent knowledge in a machine-readable way, allowing us to talk about anything in the form of resource-property-value triples. In this way, RDF is a universal language that lets users describe resources using their own vocabularies. However, RDF doesn’t make any assumptions regarding the meaning of these resources. In the previous example, the only assumption that is made is that Parthenon is a resource (since it is assigned a URI). Nothing is expressed regarding what Parthenon is. To do so, we need to build vocabularies (i.e. groups of terms along with their definitions, also known as controlled vocabularies -CVs), hierarchies of terms (i.e. taxonomies) or ontologies. In other words, we need to extend RDF with the ability to define our own schemas for describing the domains of knowledge we need to represent. This is accomplished by [RDF Schema (RDFS)](https://www.w3.org/TR/rdf12-schema/), an extension of RDF, implemented as a set of URIs that are universally declared by W3C  (World Wide Web Consortium) and define -among others- the following concepts:

* A class can be thought as a set of elements (rdfs:class).&#x20;
* Individual objects that belong to a class a referred to as instances of the class (rdfs:type).&#x20;
* Restrictions can be imposed by specifying the domain and range of the class (rdfs:domain, rdfs:range).&#x20;
* Hierarchies and inheritance among the classes and properties can be modeled (rdfs:subClassOf, rdfs:subPropertyOf).&#x20;
* Human friendly labels can be associated with a resource (rdfs:label).

Using these properties we can:&#x20;

1. define concepts as classes of specific meaning
2. assign meaning to resources, by declaring them as instances (types) of a particular class
3. describe rules for a resource in order for it to belong to a specific class
4. declare structure among classes and thus inheritance of their attributes (e.g. every instance that belongs to a class inherits all the attributes of its superclass as well)&#x20;
5. represent the URIs in a human friendlier manner using simple labels.

An example of the functionalities that can be achieved with RDFS in terms of knowledge representation can be demonstrated in the following figure:

<Figure src="/assets/content/assets/en/resources/hosted/a-practical-introduction-to-knowledge-graphs/rdfs-example.png" alt="rdfs example" alignment="stretch">

</Figure>

Following the previous statement, using the predefined terms of RDFS we can create an ontology that defines the classes *Monument*, *ArtificialObject*, *City*, *Country* and *Region* as well as the properties *locatedInCity* and *locatedIn* along with their assigned domains and ranges respectively. In addition, we can define the attribute *ConstructionDate* with range the literal (date) and domain the class *ArtificialObject*. We can also define hierarchies using the rdfs properties: *subClassOf* and *subPropertyOf* in order to impose inheritance of properties and attributes among classes.

So, in our initial statement “Parthenon is Located in Athens” each resource can be modeled, using the (*rdfs:typeOf*), as instance of the classes: *Monument*, *LocatedIn* and *City* respectively. Using this language, a machine that parses the above triplets of our KG will be able to understand that Parthenon is a monument which is a type of building that is located in Athens which is a City of Greece, etc. This way we can declare arbitrarily complex schemas representing knowledge of various domains and then provide actual data as instances of those classes and relationships so that computers, parsing our data, can understand not only the information itself but also the meaning of it.

#####

<Callout kind="note" title="RDFS Properties for designing schemas:">
  * **rdfs:type:** This property declares a resource (subject) is an instance of a class (object), essentially saying "this thing is a type of that category," For example the triple: John rdfs:type foaf:Person means John is a person, linking data to its schema definitions for meaning and inference.
  * **rdfs:Class, rdfs:Property** and **rdfs:Literal:** These properties are used to define classes of resources, relationships among resources and literal values (such as text, numbers and dates) respectively.
  * **rdfs:subClassOf:** This property defines a hierarchy between classes, indicating that one class is a subclass of another. For example, if Man is a subclass of Person, then any instance of Man is also an instance of Person.&#x20;
  * **rdfs:domain** and **rdfs:range:** These properties specify the classes of entities that can be subjects and objects of a particular property, respectively. This helps define the scope of relationships between entities.&#x20;
  * **rdfs:subPropertyOf:** This property establishes a hierarchical relationship between properties, indicating that one property is a subproperty of another. For instance, if *hasName* is a subproperty of *hasProperty*, then any entity related by *hasName* is also related by *hasProperty*.&#x20;
  * **rdfs:label** and **rdfs:comment**: These properties provide human-readable labels and comments for entities, which can be helpful in understanding the relationships between entities.&#x20;
  * **owl:ObjectProperty** and **owl:DatatypeProperty**: These properties define the types of relationships between entities and their associated values.&#x20;
</Callout>

### Syntax Formats

As with every language, syntax provides a way to “serialize” what is articulated, in a specific format. In computer languages like RDF and RDFS, syntax formats transform an abstract model into a serialized sequence of symbols that can be stored in a specific -machine readable- file, the same way a text in a word processor can be serialized into .doc, .docx or .lib file, an image into a .jpg file, etc.. For the Semantic Web family of languages, the most common syntax formats are:

* Turtle (Terse RDF Triple Language) that uses the extension .ttl for the serialized files. It is a text-based syntax for RDF where URLs are enclosed in angle brackets. The subject, property, and object of a statement appear in order, followed by a period.&#x20;
* RDF/XML, is an encoding of RDF in XML language allowing it to be parsed by browsers and other XML processing software. It is one of the first syntaxes of RDF that was gradually replaced by the easier-to-read Turtle.&#x20;
* RDFa, a syntax format specifically developed for enabling RDF to describe or markup the content of HTML web pages by embedding RDF within HTML tags.&#x20;
* OWL/XML, an XML syntax specifically developed in order to capture the functional-style-syntax that relates more to the formal structure of ontologies but at the same time being readable by XML parsing tools.&#x20;
* Manchester, originally developed by the University of Manchester and designed to be as human-readable as possible. Currently can be used by ontology editors such as [Protége](https://protege.stanford.edu/).

### Use Case: Creating an Ontology for research activities

In the following example we will demonstrate the basic functionalities of RDFLib by designing our own KG. Specifically, we will focus in i) creating a domain ontology by declaring our classes and properties and ii) serializing our schema into a file. Our use case will be the creation of a KG regarding scholarly work in humanities. In order to do so we will borrow a few concepts from Scholarly Ontology (SO), a conceptual framework specifically designed to document scholarly activities, as they appear in research articles.

For the purposes of our use case we will build a small ontology consisting of the core concepts and relationships of SO, i.e. those of i) Activity, describing any research activities or parts thereof (such as an archaeological excavation, a biological experiment, a social study, etc.); ii) Method, denoting any plan, technique or procedure that describes how the activity was conducted; iii) Person, denoting any researcher that participates in an activity and iv) Article, denoting the research article where the above instances were derived from. This schema is illustrated in the following figure:

<Figure src="/assets/content/assets/en/resources/hosted/a-practical-introduction-to-knowledge-graphs/exersice-ontology.png" alt="ontology example" alignment="center">

</Figure>

In this example, we define only one direction for each property of our ontology for readability purposes. In reality, depending on the information needs, we could also create a separate property for each direction of our semantic relationships (e.g. employs(Activity,Method) and isEmployedBy(Method,Activity), etc.). Creating specific properties for each direction can help for faster query execution when the specific semantic direction of the graph traversal is needed but increases the need for storage since, we add more triples to our KG.

---

## Populating Knowledge Graphs

### Datasets as source for Knowledge Graph population

As mentioned in the previous section, a Knowledge Graph (KG) is an organized representation of data in the form of entities and their relationships. Along this line, in order to build a KG we first need to gather data out of which we will create its instances. Instances of a KG refer to the actual objects that populate the KG wheter they are entities ore relationships. &#x20;

<Callout kind="tip">
  Data gathering can be achieved through various ways. Indicatively, we can use Application Programming Interfaces (APIs) and retrieve data from publishing repositories such as [Springer](https://www.springeropen.com/get-published/indexing-archiving-and-access-to-data/api), [Elsevier](https://dev.elsevier.com/), [Constellate](https://constellate.org/builder?unigrams=patients,%20health), etc.; we can use database endpoints and retrieve our data using a query language (e.g. SPARQL queries in an RDF repository like [DBPedia](https://www.dbpedia.org/), [Wikidata](https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service), etc.); or we could download entire datasets from the web (e.g. through websites like [Kaggle](https://www.kaggle.com/), [Data.Gov](https://data.gov/), or using [Google Dataset Search](https://datasetsearch.research.google.com/), etc.). Alternatively, we could exploit the structured content of web sites and gather our data through a technique called web scraping.&#x20;
</Callout>

After gathering our data we need to parse them and extract the clean text preferably segmented into sentences, paragraphs or sections. The parsed text can be used as a source for aquiring extra information and creating instances for our KG.

### Information Extraction&#x20;

After aquiring our dataset, the next step is to extract information from it which can then be transformed into instances for our Knowledge Graph. This process is called information extraction from text and can be achieved through various ways ranging from simple rule-based methods to more sophisticated machine learning (ML) or deep learning (DL) techniques.&#x20;

Starting with the parsed text in the input, the basic steps of the information extraction workflow are can be seen in the followung figure:

<Figure src="/assets/content/assets/en/resources/hosted/a-practical-introduction-to-knowledge-graphs/ssh-workflow.png" alignment="center">

</Figure>

#### Entity Extraction

Entity Extraction from text involves the recognition and extraction of textual spans that can be associated with a specific type of entity. Examples of extracted entities include phrases in text denoting dates, historic events, persons' names, locations, organizations, etc. The extraction of such entities varies in difficulity based on the lexical complexity of the entity type and usually involves the employment of ML or DL models that are previously trained (or fine tuned in case of DL models) in extracting the specific type of entity from text. Along this line, entity extraction is usually treated as a token classification or sequence labelling task where a classifier (trained ML or DL model) predicts whether each token in the input belongs to the entity (or entities) in question or not.

<Callout kind="note" title="A word on Tokens">
  In machine learning, a token is the basic unit of data that an AI model processes, essentially breaking down text or other inputs into manageable pieces like words, sub-words, or characters, allowing the model to understand patterns, context, and meaning for tasks like language generation or analysis. Depending on the tockenizer and the purposes of the task, tokens can be:

  * **Words:** Simple splitting by spaces (e.g., "Hello world" becomes \["Hello", "world"]).
  * **Sub-words/Parts of Words:** More advanced tokenizers break down complex words (e.g., "tokenization" might become "token" + "ization") to handle rare words better.
  * **Characters:** Individual letters, numbers, or symbols.
  * **Punctuation:** Commas, periods, and other symbols are often separate tokens
</Callout>

#### Entity Disambiguation

Entity disambiguation (ED) is **a core Natural Language Processing (NLP) task that resolves ambiguity by linking mentions of real-world things (like people, places, organizations) in text to their unique, correct entry in a knowledge base, preventing confusion between homonyms** (e.g., "Apple" the fruit vs. "Apple" the company). It's crucial for accurate information extraction, semantic search, and AI systems, allowing them to understand context and identify specific entities, even when names are common or shared. Usually tntity disambiguation is conducted in **named entities** (i.e. entities that can be found in literature under the same proper name or variations of it) and perform the disambiguation by associating them with their corresponding entry in a Knowledge Base such as Wikipedia.

<Callout kind="note" title="Knowledge Base vs Knowledge Graph">
  A **knowledge base** is a broad term for a repository of information (structured or unstructured) for easy access, while a **knowledge graph** is a specific *type* of knowledge base that uses a graph structure (nodes and edges) to explicitly represent entities and their semantic relationships, enabling complex reasoning by machines for AI tasks like intelligent search and recommendations. Essentially, all knowledge graphs are knowledge bases, but not all knowledge bases are knowledge graphs; **the key difference is the emphasis on interconnected, meaning-rich relationships in a graph**. 
</Callout>

Entity disambiguation systems are usualy ML or DL models trained to associate the extracted entity with the apporpriate entry in a KB out of a set of generated candidates. This association is done based on the context of the extracted entity such as the sentence, phrase or even paragraph that the entity was extracted from. Typically the steps for entity disambiguation (after the entity is extracted) are the following:&#x20;

* **Generates Candidates:** The system generates possible real-world entities (for example, for the extracted entity "Washington", the system witll generate candidate links from the Knowledge base (like Wikipedia) such as Washington State, Washington D.C., George Washington).
* **Uses Context:** The system analyzes the surrounding text (context) of the extracted entity and the relationships / textual descriptions within the KB for each of the candidates in order to determine which candidate is the intended one.

#### Linking

Linking is the process that produces links among οbjects in order to associate them in some semantic way. These objects can be mentions of entities  that are extracted from text (i.e. textual spans that refer to a specific entity), entries from a Knowledge Base such as Wikipedia, metadata elements from varius repositories, etc.&#x20;

Especially for Semantic Web, there are several properties in [RDFS](https://www.w3.org/TR/rdf-schema/) and [OWL](https://www.w3.org/TR/owl-features/) languages that are specifically designed for producing varius semantic links among resources depending on whether they are Classes, individuals (i.e. instances of the KG) or properties (i.e. relationships among resources).

<Callout kind="note" title="SW Properties for  linking:">
  * **owl:equivalentClass**: This property indicates that two classes are considered to be semantically equivalent.&#x20;
  * **owl:sameAs**: This property is used to assert that two individuals are the same entity.
  * **owl:disjointWith**: This property indicates that two classes have no common instances.&#x20;
  * **owl:intersectionOf**, **owl:unionOf**, and **owl:complementOf**: These properties are used to define complex class descriptions by combining simpler classes.
</Callout>

#### Relation Extraction

Relation (or relationship) Extraction (RE) is the task of identifying and extracting semantic relationships from a text. Extracted relationships usually occur between two or more entities of a certain type (e.g. Person, Organisation, Location, etc.) and fall into a number of semantic categories (e.g. isFriendOf, isEmployedBy, livesIn, etc.). In this sence, RE is crucial for building knowledge graphs and powering applications like question answering and search, moving from unstructured text to structured knowledge.&#x20;

As with other IE tasks, RE can be achieved using methods ranging from rule-based systems, to ML / DL pretrained models or even by prompting Large Language Models (LLMs). Depending on the methodology, varius aspects are examined in order to extract a relation. For example, in rule-based aproaches, usualy the textual characteristics of the extracted entities play a key role in defining the rules for relation extraction. Such characteristics are, for example, to check whether the extracted entities  overlap, if they are included in the same sentence/paragraph, if they contain any specific lexical indicators, etc..  In ML / DL apporaches extracting relations is usualy treated as a text classification problem. This requires examining all entity pairs. Specifically, for every pair of extracted entities, the text chunk bounded by these two entities, \[entity1, ..., entity2], is treated as expressing a candidate relation. In order to restrict the search to a reasonable set of candidates but also to allow for possible coreferences among sentences to be resolved, a maximum limit (e.g. the bounding entities must be in the same paragraph or in the same sentence) is set for candidate creation. A classifier then determines whether the bounding entities of the chunk satisfy the relation in question.&#x20;

<Callout kind="note" title="Using Machine Learning Methods">
  Using Machine Learning methods for a specific task requires ML models that are trained approprietly in order to be able to perform for the task at hand.

  To train a ML model, we need training data, i.e. examples of text along with the labels we want the model to predict. When training a ML model, we don’t just want it to memorize our examples – we want it to come up with a “theory” that can be generalized across unseen data. That’s why the training data should always be representative of the data we want to process. A ML model trained on Twitter, for example, will most likely perform badly on research articles, etc.

  This also means that in order to know how the model is performing, and whether it’s learning the right things, we don’t only need training data but also need evaluation data. If we only test the model with the data it was trained on, we’ll have no idea how well it’s generalizing to new stuff. If we want to train a model from scratch, we usually need at least a few hundred examples for both training and evaluation. In general the size of the training / evaluation datasets can vary from few hundreds to thousands, depending on the complexity of the task at hand and the types of ML models to be used.
</Callout>



<Callout kind="important" title="Evaluation metrics">
  The most common way to evaluate a ML model is by measuring Precision, Recall and F1 scores. Each score measures different aspects of the performance of the model.&#x20;

  To demonstrate the use of each metric, let’s say that we have created a ML classifier for recognizing research methods’ names. In order to evaluate the performance of our ML model, we have created an annotated dataset of 100 sentences with a total of 1000 tokens that constitutes the gold standard. During evaluation, our Methods classifier will parse each token in the input and predict whether it is part of a researrch method’s name or not. At the end we will compare the predictions of our classifier with the gold standard and based on this comparison we will calculate the following scores:

  **Precision** is about **accuracy among the selected**. Out of all the tokens our model labeled as METHOD, how many actually were? If our classifier finds 100 tokens, but only 80 of them are actually parts of a research method name, then:&#x20;

  * **Precision = 80/100 = 0.8 or 80%**&#x20;
  * We got 20 false positives (tokens not part of a method name that were wrongly labeled as methods).

  **High precision = fewer false positives.**

  **Recall** is about **completeness**. Out of all the tokens that are indeed method names in the dataset, how many did our model successfully predict? If there are actually 200 tokens that should be labeled as METHOD in the dataset, and our model found 80 of them:

  * **Recall = 80/200 = 0.4 or 40%**&#x20;
  * We missed 120 tokens—false negatives.&#x20;

  **High recall = fewer false negatives.**

  The **F1 Score** is the **harmonic average of precision and recall**. Think of it as a way to balance both measures when you don’t want to prioritize one over the other. If precision is about being right, and recall is about being thorough, F1 is about being both. It penalizes you if you're great at one but bad at the other. In our example:&#x20;

  * Precision = 0.8&#x20;
  * Recall = 0.4 Then:&#x20;
  * **F1 Score = 2 × (0.8 × 0.4) / (0.8 + 0.4) = 0.533 or 53.3%**
</Callout>



---

## Publishing and querying KGs for the Semantic Web

As we explored earlier, the main idea behind the Semantic Web vision was to publish structured information (not just datasets but each individual data element) by describing their intended semantics as a directed, labeled graph of inter-related instances, so that it can be processed by machines.&#x20;

The implementation of the above principles led to the creation of the RDF RDFS and OWL data models that provide the frameworks for building “inter-related instances” and the SPARQL language for querying them. In fact, this concept of iter-relating published data where each data element can be accessed individually through basic web protocols is what led to the idea of “Linked Data”. The latter can be seen as a set of best practices for publishing data on the web in a machine-readable, interconnected format with the goal to create a global graph of data where different datasets can be linked together and accessed easily. More specifically the Linked Data principles can be summarized as follows:

1. Use URIs as names for things
2. Use HTTP URIs so that people can look up those names.
3. When someone looks up a URI, provide useful information, using the standards (RDF, SPARQL)
4. Include links to other URIs so that they can discover more things.

Let’s dive into each of these principles one by one to make them clearer: &#x20;

### The Anatomy of a URI

**Uniform Resource Identifiers** are a crucial element used to uniquely identify resources, including both physical and abstract objects when it comes to publishing information for the Semantic Web. In addition, the use of -commonly used by all web servers- HTTP protocol in order to create those URIs allows for easy adaptation and recognition of those identifiers by any server in the web. Focusing on the anatomy of a URI this can be decomposed into the following parts:

![the anatomy of a URI](/assets/content/assets/en/resources/hosted/a-practical-introduction-to-knowledge-graphs/image.png)



**Protocol:**
This specifies the protocol or mechanism used to access the resource, such as http, https, or urn.
**Domain:**
This part indicates the domain or server that hosts the resource
**Path:**
This specifies the location of the resource within the domain, often using a directory structure and file name.
**Local Unique Identifier:**
This is an optional part that identifies a specific portion or sub-resource within the primary resource.

The protocol, along with the domain and the path, constitute the namespace of the URI. The latter is separated by the local unique identifier using -preferably- a hashtag or a backslash in order to discern it from the rest of the path. Following this structure, we can create URIs for all of our resources which will constitute the instances of our Knowledge Graph. In fact, this structure is quite similar to a webpage URL with the local unique identifier being the slug. As such -in accordance with guidelines 2 and 3- individual webpages or even sections on pages could be associated with and thus provide useful information for the corresponding URIs.

### The SPARQL query language

So far, using the RDF and RDFS vocabularies, we are able to represent any kind of knowledge regarding digital objects themselves or even the types they belong to in the form of Knowledge Graphs for the Semantic Web. Access to this structured knowledge can be achieved through queries, i.e. encoded requests that enable users to retrieve information from databases. In fact, when designing storage applications for any type of data, the basic functions that are considered necessary are those for Create, Read, Update and Delete data elements (CRUD). Along this line, any language developed for querying a storage interface (i.e. endpoint) must support these basic operations. When it comes to retrieving and manipulating data stored in Resource Description Framework format, the SPARQL query language has been developed. This allows for performing any of the CRUD operations in knowledge graphs for the Semantic Web while at the same time using the structure of the internet protocols for communication. In particular, the main idea behind storing and retrieving data in Semantic Web can be summarized as follows:

•	Use triple stores to store RDF data (accessible through Endpoints)
•	Use SPARQL queries to access or update the triple store (CRUD operations)
•	Communication with the triple store can be achieved through HTTP protocol

As with any other language, queries in SPARQL must follow a specific format. An indicative example demonstrating the basic sections of a SPARQL query can be seen in the figure below:

Every SPARQL query must consist at least of two sections: 1) the section where we specify the type of the result we want and the variables to appear in the query results (query result clause) and 2) the section where we specify the patterns, we want the query to match against (query pattern). In our example we use the SELECT DISTINCT clause to declare that we want to perform a Retrieve operation and we are interested in only distinct values of the variable ?name inside the graph (each variable name is declared using the “?” prefix). In addition, we use the WHERE clause to declare inside curly brackets that the pattern we want to match inside the graph consists of any two variables that are connected with each other through the relationship foaf:name.

For readability purposes we use the shortcut foaf instead of the entire URI of the relationship (http\://xmlns.com/foaf). This shortcut declaration is optional and can take place in the beginning of the query using the PREFIX clause. Another optional declaration can take place at the end of the query using various modifiers that describe how the retrieved results should be presented (e.g. in which order, how many, etc.). In our example, we use two modifiers: the first (“order by”) to declare that we want the results (values of the variable ?name) to be presented in alphabetical order and the second (“limit”) to declare that we want to display only the first 10 of those results.

A SPARQL query can be executed in a triple store (i.e. a repository for storing Knowledge Graphs in the form of RDF triples) via a SPARQL endpoint (i.e., a specific interface for connecting with triple stores and executing any CRUD operation with SPARQL). Running it would cause the traversal of the KG and the retrieval of all the values that match the specified pattern (i.e. are connected with any resource through the foaf:name property in our case). After the values are retrieved, they are sorted in alphabetical order and then the first ten are displayed to the user (as specified by the modifiers order by and limit respectively). For a thorough exploration of the SPARQL language you can visit the official SPARQL W3C website.











